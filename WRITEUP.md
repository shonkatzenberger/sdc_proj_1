# **Finding Lane Lines on the Road** 

Shon Katzenberger  
shon@katzenberger-family.com  
October 11, 2017

---

## 0. Terminology ##

First, let's agree on some terminology. An idealized **lane line** is not a line in a mathematical sense, but
is a portion of some curve. Nevertheless, we use the term **lane line** to mean such a curve.
However, the term **line** *does* mean a mathematical line, extending forever in opposite directions.
A **segment** (or **line segment**) is a contiguous portion of a **line** of finite, positive length.
From basic geometry, a **segment** uniquely determines a **line**.

Note also that **we** and **our** really mean **I** and **my**. In other words, this is solo work, as expected.

## 1. Pipeline Description

The image pipeline is encapsulated in our `processImage(pict, enhance=True, useAngles=False)` function.
It consists of the following steps:
* Copy the input image (to avoid modifying the input).
* If the `enhance` parameter is `True` (its default), invoke the `enhanceColors` function on the image copy.
This identifies pixels whose color is close to an expected line color (white or yellow) and replaces them with pure white.
This step helps with the challenge video.
* Convert the (possibly enhanced) image to gray scale, apply Gaussian blur, and apply Canny edge detection. The result is
a gray-scale image, named `canny`.
* Generate a black-and-white trapezoidal region mask. The parallel edges of this region are, of course, horizontal.
Apply this mask to `canny` by using the `numpy.minimum` function, forcing pixels of `canny` that are outside the region
to be black.
* Detect/generate line segments in the masked `canny` image using OpenCV's HoughLines functionality.
* Use the generated line segments to compute (at most) two lines that approximate the lane lines (curves).
See below for details on how these lines are computed. Also, compute the intersection of these lines with
the bottom and top lines of the trapezoidal region, and render the resulting segments into a separate,
initially blank, image.
* Blend the lane line image with the original image to produce the result.

### Lane Line Computation

The line segments generated by HoughLines fall into three categories:
1. Part of the left lane line.
2. Part of the right lane line.
3. Not part of either lane line.

It's convenient to choose our coordinate system such that the origin is the bottom-center of the image, positive x
is to the right, and positive y is up. We convert the segments to this coordinate system, and use
this coordinate system in the rest of this discussion.

Assuming the two lane lines are relatively straight, we expect segments in each of (1) and (2) to
lie on (close to) the same (infinite) line. Therefore, the lines defined by these segments should have
similar geometric features, such as:
* Slope, or, the **angle** the line makes with the positive x-axis.
* The **distance** from the line to the origin.
* The **intercepts**, or x-values of the intersections, of the horizontal lines `y = 0` and `y = yTop`, where
`yTop` is the y coordinate of the top of the trapezoidal region.

Our strategy is to compute (some of) these geometric features and use them to identify segments that are
part of each lane line, with reasonable confidence. We then compute weighted averages of the features
and use those averages to choose our idealized lines.

We wrote two implementations, one using angle and distance, and the other using intercepts.
They both perform reasonably well. The latter seems to have slightly less jitter and do a better job of
choosing the `y = 0` intercept.

For both implementations, we **normalized** the segments so the second point has the larger y-coordinate,
by swapping the two points if that isn't the case initially.

#### Angle-Distance Method

The angle-distance implementation requires a bit of math. The angle is computed using `atan2` applied to
the differences of the point coordinates. Because of the normalization, this angle is always between
0 and pi.

Rather than distance (always non-negative), we actually use the **signed distance**, which we now define.
It's well know that the **signed area** of a parallelogram defined by two vectors,
`v0 = (x0, y0)` and `v1 = (x1, y1)`, is simply the (z-component of) the cross product of those vectors,
namely, `v0 x v1 = x0 * y1 - x1 * y0`. The area is the absolute value of this. The sign indicates whether the triangle
determined by the origin, `v0`, and `v1` is clockwise or counter-clockwise. It's easy to see that the area
of the parallelogram is also the length of the segment `(v0, v1)` times the distance from the origin to the
line determined by the segment (draw the picture and note that the distance is the length of an altitude of
the triangle). This suggests defining the **signed distance** as `(v0 x v1) / |v1 - v0|` where `|v1 - v0|`
is the length of the segment.

Generally, we expect this signed distance to be negative for portions of the left lane line and positive
for the right lane line. We also expect portions of the left lane line to have angles between 0 and pi/2
and portions of the right lane line to have angles between pi/2 and pi. In practice we narrow these ranges
to only select segments with *reasonable* angles. We use these criteria to select our candidate segments
for each lane line.

When computing averages, we weight the angle and signed distance of each segment by the length of the segment.
Intuitively, longer segments should count more.

#### Intercept Method

The intercept method uses the x-values of the intersections with the horizontal lines `y = 0` and `y = yTop`.
Of course horizontal lines are not selected as candidates for either lane line. For the left lane line we expect
candidate segments to have a negative bottom intercept, while right lane line candidates should has a positive
bottom intercept. We also impose additional reasonable bounds on the both the bottom and top intercepts.

When computing averages, we weight the intercepts by the product of the normalized vertical span of the
segment, namely, `(y1 - y0) / yTop`, and `1` minus the normalized distance from the *segment* to the
corresponding horizontal line, namely, `y1 / yTop` for the top intercept, and `1 - y0 / yTop` for
the bottom intercept. Intuitively, segments that are *closer* to the horizontal line should count more,
and segments that are *taller* should count more.

#### Common details

For either method, for each side (left and right), and for each feature, we compute the weighted mean and variance
of the feature, and, and as long as the variance is above some threshold, trim an outlier value and repeat.
From this we get our estimates of the features for the idealized lines.

From these feature values, we compute the intercepts, and draw the final two segments.

Note: we also experimented with trimming entire segments from the candidate pool when one of their features
was an outlier, rather than trimming just the outlier feature values. We found this produced inferior results,
especially when, as in the challenge video, the road curves.

## 2. Potential Shortcomings of the Current Pipeline

There are several obvious shortcomings of this work:
* The parameters and thresholds are tuned for bright daylight. Different lighting conditions would
affect at least the color enhancement step, but likely many of the other tuning parameters, such as
the parameters for Canny edge detection.
* This depends on dry, well-marked pavement. If there is a high degree of reflection or the markings
are somewhat faded or obscured, we'll likely get low quality results from the enhance/Canny/Hough steps.
* The Canny/Hough steps don't consider the actual color. That is, a bright purple line is as likely to get
detected as a bright yellow one.
* The Canny/Hough steps can also pick up the edges of *dark* lines, as in the tree shadows in the challenge video.
* There is significant jitter, especially when the markings are segmented. As the vehicle progresses, the algorithm
moves from segment to segment getting slightly different *feature* values. A possible solution is to
smooth over time. This requires carrying state from one frame to the next, so would require slightly
different invocation code. Rather than `result = clip.fl_image(process_image)`, we could do something like:

    ```
    func = getImageProcessor()
    result = clip.fl_image(func)
    ```

    The `func` can then be a closure that captures state needed to smooth between frames.

* Fitting a straight line segment to a curved road produces poor results, as in the challenge video.
This is especially true when the pavement markings are segmented, when the segmented jitter mentioned
above becomes extreme. As documented in the code, one potential approach would be to fit a circular arc,
rather than a straight line segment. Note that a straight line segment is the special case of an arc
with infinite radius. Of course, such an implementation would need to carefully guard against overflow,
when the path is nearly straight. Another possibility is generating a polyline. This has the advantage
of not requiring constant curvature, but would become significantly more complex.
* The region selection is sensitive to road 'grade' changes. For example, when the road slope is
increasing (2nd derivative is positive), the region should/could be taller, and when the road slope is
decreasing, (2nd derivative is negative), the region should/could be shorter.
* To control a vehicle, that is, decide how much to turn by, etc, line computations like this are
not sufficient. We'll really need estimates projected onto the model of the road, not onto a *screen*.

## 3. Suggest possible improvements to your pipeline

Possible improvements include:
* Smoothing between frames, as described above. This requires modification to the harness code.
* Model lane lines as circular arcs or 'poly-lines', to improve results on curves.
* Explore using different color spaces, as well as applying Canny/Hough to each color channel separately,
and aggregating results.
* Improve memory efficiency. This isn't a concern in off-line batch processing like the notebook does,
but would be significant in a real deployment. For example, re-use buffers rather than constantly allocating
new ones.
* Explore auto-tuning of parameters. For example, attempt multiple parameter settings with some feedback to
direct ideal choice. Feedback could include the variance of estimates, number of segments detected, consistency
from frame to frame, etc. This could help with varying lighting conditions.
